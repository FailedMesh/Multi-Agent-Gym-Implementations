{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44771e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef5db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ma_gym:PredatorPrey5x5-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc020a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b23af1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1.], (28,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8ffb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.prey_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab4c4586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: None, 1: None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.agent_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57cf015e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb008f",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d0321",
   "metadata": {},
   "source": [
    "## Environment Dynamics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f4273e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_encoding(state):\n",
    "    encoding = 0\n",
    "    for i in range(len(state)):\n",
    "        encoding += (5**(5-i))*int(state[i])\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f71a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_to_state(encoding):\n",
    "    state = \"\"\n",
    "    for i in range(5, 0, -1):\n",
    "        remainder = encoding % 5\n",
    "        encoding = encoding//5\n",
    "        state = str(remainder) + state\n",
    "    remainder = encoding % 5\n",
    "    state = str(remainder) + state\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40be2235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_to_encoding(action):\n",
    "    encoding = 0\n",
    "    for i in range(len(action)):\n",
    "        encoding += (5**(1-i))*int(action[i])\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a218f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_to_action(encoding):\n",
    "    action = \"\"\n",
    "    remainder = encoding % 5\n",
    "    encoding = encoding//5\n",
    "    action = str(int(remainder)) + action\n",
    "    remainder = encoding % 5\n",
    "    action = str(int(remainder)) + action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10741ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state, action):\n",
    "    \n",
    "    #Encoding in general means the number, state is the string\n",
    "    state = encoding_to_state(state)\n",
    "    action = encoding_to_action(action)\n",
    "    \n",
    "    good_noop = 0\n",
    "    noop_1 = 0\n",
    "    noop_2 = 0\n",
    "    \n",
    "    #STATE:\n",
    "    #First 2, Second 2, Third 2, depict the positions of the 1st pred, 2nd pred, and prey respectively.\n",
    "    \n",
    "    #PREDATOR 1:\n",
    "    \n",
    "    if action[0] == \"0\":\n",
    "        if state[0] != \"4\":\n",
    "            state = state[:0] + str(int(state[0]) + 1) + state[1:]\n",
    "    if action[0] == \"1\":\n",
    "        if state[1] != \"0\":\n",
    "            state = state[:1] + str(int(state[1]) - 1) + state[2:]\n",
    "    if action[0] == \"2\":\n",
    "        if state[0] != \"0\":\n",
    "            state = state[:0] + str(int(state[0]) - 1) + state[1:]\n",
    "    if action[0] == \"3\":\n",
    "        if state[1] != \"4\":\n",
    "            state = state[:1] + str(int(state[1]) + 1) + state[2:]\n",
    "    if action[0] == \"4\":\n",
    "        if (abs(int(state[4]) - int(state[0])) == 1 and abs(int(state[5]) - int(state[1])) == 0) or (abs(int(state[4]) - int(state[0])) == 0 and abs(int(state[5]) - int(state[1])) == 1):\n",
    "            noop_1 = 1\n",
    "        \n",
    "    #PREDATOR 2:\n",
    "    \n",
    "    if action[1] == \"0\":\n",
    "        if state[2] != \"4\":\n",
    "            state = state[:2] + str(int(state[2]) + 1) + state[3:]\n",
    "    if action[1] == \"1\":\n",
    "        if state[3] != \"0\":\n",
    "            state = state[:3] + str(int(state[3]) - 1) + state[4:]\n",
    "    if action[1] == \"2\":\n",
    "        if state[2] != \"0\":\n",
    "            state = state[:2] + str(int(state[2]) - 1) + state[3:]\n",
    "    if action[1] == \"3\":\n",
    "        if state[3] != \"4\":\n",
    "            state = state[:3] + str(int(state[3]) + 1) + state[4:]\n",
    "    if action[1] == \"4\":\n",
    "        if (abs(int(state[4]) - int(state[2])) == 1 and abs(int(state[5]) - int(state[3])) == 0) or (abs(int(state[4]) - int(state[2])) == 0 and abs(int(state[5]) - int(state[3])) == 1):\n",
    "            noop_2 = 1\n",
    "            \n",
    "    reward_array = np.zeros((4,))\n",
    "    state_array = [state, state, state, state]\n",
    "    prev_state_array = state_array[:]\n",
    "    \n",
    "    #All Possible Next States:\n",
    "    \n",
    "    if state_array[0][4] != \"4\":\n",
    "        state_array[0] = state_array[0][:4] + str(int(state_array[0][4]) + 1) + state_array[0][5:]\n",
    "    if state_array[1][5] != \"0\":\n",
    "        state_array[1] = state_array[1][:5] + str(int(state_array[1][5]) - 1)\n",
    "    if state_array[2][4] != \"0\":\n",
    "        state_array[2] = state_array[2][:4] + str(int(state_array[2][4]) - 1) + state_array[2][5:]\n",
    "    if state_array[3][5] != \"4\":\n",
    "        state_array[3] = state_array[3][:5] + str(int(state_array[3][5]) + 1)\n",
    "    \n",
    "            \n",
    "    #TERMINAL CASES:\n",
    "    \n",
    "    for i in range(len(state_array)):\n",
    "\n",
    "#         if action[0] == \"4\":\n",
    "#             if abs(int(state_array[i][4]) - int(state_array[i][0])) == 1 or abs(int(state_array[i][5]) - int(state_array[i][1])) == 1:\n",
    "#                 noop_1 = 1\n",
    "\n",
    "#         if action[1] == \"4\":\n",
    "#             if abs(int(state_array[i][4]) - int(state_array[i][2])) == 1 or abs(int(state_array[i][5]) - int(state_array[i][3])) == 1:\n",
    "#                 noop_2 = 1\n",
    "        \n",
    "        if (noop_1 + noop_2) == 2:          #Correct Noop\n",
    "            state_array[i] = state_to_encoding(state_array[i])\n",
    "            reward_array[i] = 1\n",
    "        elif (noop_1 + noop_2) == 1:        #Wrong Noop       \n",
    "            state_array[i] = state_to_encoding(state_array[i])\n",
    "            reward_array[i] = -0.5\n",
    "        elif (action[0] == \"4\" and noop_1 == 0) or (action[1] == \"4\" and noop_2 == 0):  #Terrible Noop\n",
    "            state_array[i] = state_to_encoding(state_array[i])\n",
    "            reward_array[i] = -1\n",
    "        #Collision:\n",
    "        elif (state_array[i][:2] == state_array[i][2:4]) or (state_array[i][2:4] == state_array[i][4:]) or (state_array[i][:2] == state_array[i][4:]):\n",
    "            state_array[i] = state_to_encoding(prev_state_array[:][i])\n",
    "            reward_array[i] = -0.5\n",
    "        #Movement Collision:\n",
    "        elif (state_array[i][:2] == prev_state_array[i][2:4] and prev_state_array[i][:2] == state_array[i][2:4]) or (state_array[i][4:] == prev_state_array[i][2:4] and prev_state_array[i][4:] == state_array[i][2:4]) or (state_array[i][:2] == prev_state_array[i][4:] and prev_state_array[i][:2] == state_array[i][4:]):\n",
    "            state_array[i] = state_to_encoding(prev_state_array[:][i])\n",
    "            reward_array[i] = -0.5\n",
    "        else:\n",
    "            state_array[i] = state_to_encoding(state_array[i])\n",
    "            reward_array[i] = -0.01\n",
    "        \n",
    "    \n",
    "    return reward_array, state_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae4d0130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4544, 4538, 4534, 4539] [0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'121123'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_action = action_to_encoding(\"31\")\n",
    "encoded_state = state_to_encoding(\"111224\")\n",
    "reward, next_state = get_next_state(encoded_state, encoded_action)\n",
    "print(next_state, reward)\n",
    "encoding_to_state(next_state[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b161d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = \"001144\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bf3024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0-->Down\n",
    "#1-->Left\n",
    "#2-->Up\n",
    "#3-->Right\n",
    "action_dict = {0:\"Down\", 1:\"Left\", 2:\"Up\", 3:\"Right\", 4:\"Noop\"}\n",
    "\n",
    "encoded_state = state_to_encoding(state)\n",
    "encoded_action = action_to_encoding(\"20\")\n",
    "state = get_next_state(encoded_state, encoded_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e50b7a",
   "metadata": {},
   "source": [
    "## Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "278b3858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.n = len(self.env.observation_space) + 1              # 1 prey\n",
    "        self.num_states = 5**(2*self.n)    # 5 rows/columns squared for area and multiplied by number of agents including prey\n",
    "        self.num_actions = 5*5\n",
    "        self.values = np.random.rand(self.num_states)            \n",
    "        self.policy = np.zeros(self.num_states, )\n",
    "        \n",
    "        \n",
    "    def value_iterate(self, theta = 0.01, gamma = 0.5, save = False):\n",
    "        \n",
    "        while True:\n",
    "            delV = 0\n",
    "            for state in range(self.num_states):\n",
    "                prev_value = self.values[state]\n",
    "                action_values = []\n",
    "                for action in range(self.num_actions):\n",
    "                    reward_array, next_state_array = get_next_state(state, action)\n",
    "                    next_state_vals = 0\n",
    "                    for i in range(len(next_state_array)):\n",
    "                        if next_state_array[i] == -1:\n",
    "                            next_state_vals += (0.25*reward_array[i])\n",
    "                        else:\n",
    "                            next_state_vals += (0.25*(reward_array[i] + gamma*self.values[next_state_array[i]]))\n",
    "                    action_values.append(next_state_vals)\n",
    "                self.values[state] = max(action_values)\n",
    "                delV = max(delV, abs(prev_value - self.values[state]))\n",
    "            print(delV)\n",
    "            if delV < theta:\n",
    "                #print(action_values)\n",
    "                break\n",
    "            \n",
    "                \n",
    "        for state in range(self.num_states):\n",
    "            action_values = []\n",
    "            for action in range(self.num_actions):\n",
    "                reward_array, next_state_array = get_next_state(state, action)\n",
    "                next_state_vals = 0\n",
    "                for i in range(len(next_state_array)):\n",
    "                    if next_state_array[i] == -1:\n",
    "                        next_state_vals += (0.25*reward_array[i])\n",
    "                    else:\n",
    "                        next_state_vals += (0.25*(reward_array[i] + gamma*self.values[next_state_array[i]]))\n",
    "                action_values.append(next_state_vals)\n",
    "            action_values = np.array(action_values)\n",
    "            self.policy[state] = np.argmax(action_values)\n",
    "            #print(action_values, self.policy[state])\n",
    "        \n",
    "        if save:\n",
    "            np.save(\"DP_policy.npy\", self.policy)\n",
    "\n",
    "                \n",
    "    def analyse_episode(self):\n",
    "        \n",
    "        dones = [False for _ in range(self.env.n_agents)]\n",
    "        action = [0, 0]\n",
    "        reward = [0, 0]\n",
    "        my_reward = [0, 0]\n",
    "        info = None\n",
    "        \n",
    "        _ = self.env.reset()\n",
    "        count = 0\n",
    "        \n",
    "        self.env.render()\n",
    "        _ = input(\"Press Enter to start simulation\")\n",
    "        \n",
    "        while not all(dones):\n",
    "            \n",
    "            #clear_output(wait = True)\n",
    "            self.env.render()\n",
    "            \n",
    "            a1_pos = str(self.env.agent_pos[0][0]) + str(self.env.agent_pos[0][1])\n",
    "            a2_pos = str(self.env.agent_pos[1][0]) + str(self.env.agent_pos[1][1])\n",
    "            prey_pos = str(self.env.prey_pos[0][0]) + str(self.env.prey_pos[0][1])\n",
    "            \n",
    "            state_string = a1_pos + a2_pos + prey_pos\n",
    "            state_encoding = state_to_encoding(state_string)\n",
    "            action_encoding = self.policy[state_encoding]\n",
    "            action_string = encoding_to_action(action_encoding)\n",
    "            action[0], action[1] = int(action_string[0]), int(action_string[1])\n",
    "            print(action_dict[action[0]], action_dict[action[1]])\n",
    "            print(\"\\nCurrent state in their representation = \", state_string)\n",
    "            my_reward, my_next_state = get_next_state(state_encoding, action_encoding)\n",
    "            _ = input(\"Does the action make sense? Press Enter to execute\")\n",
    "            _, reward, dones, info = self.env.step(action)\n",
    "            print(\"REWARD = \", my_reward)\n",
    "            print(\"Is agent done?: \", dones)\n",
    "            self.env.render()\n",
    "            \n",
    "    def \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0170461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "31001bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15. 24. 18. ...  6. 24.  7.]\n"
     ]
    }
   ],
   "source": [
    "agent.policy = np.load(\"DP_policy.npy\")\n",
    "print(agent.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "96fceef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7742722156571444\n",
      "0.8711045807429753\n",
      "0.7590532262640346\n",
      "0.5045372470958434\n",
      "0.3942077404325697\n",
      "0.32904660045789647\n",
      "0.2705332184909153\n",
      "0.23083617830763026\n",
      "0.20037164418457776\n",
      "0.16711729751449167\n",
      "0.13847796659839595\n",
      "0.11351792727886023\n",
      "0.09289153339497025\n",
      "0.07552232280694815\n",
      "0.06158720937712037\n",
      "0.05267876745360223\n",
      "0.04534007544833196\n",
      "0.03896131724754692\n",
      "0.03345883040189701\n",
      "0.02868752803102481\n",
      "0.024571720581408663\n",
      "0.021017932130436456\n",
      "0.017957460661794045\n",
      "0.015323904719945602\n",
      "0.013061453397187783\n",
      "0.011120191542921987\n",
      "0.00945691674725957\n"
     ]
    }
   ],
   "source": [
    "agent.value_iterate(theta = 0.01, gamma = 0.9, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdb98392",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"DP_policy_gamma001.npy\", agent.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6cc5d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7fa8d9c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press Enter to start simulation\n",
      "Right Right\n",
      "\n",
      "Current state in their representation =  223133\n",
      "Does the action make sense? Press Enter to execute\n",
      "REWARD =  [-0.01 -0.5  -0.5  -0.01]\n",
      "Is agent done?:  [True, True]\n"
     ]
    }
   ],
   "source": [
    "agent.analyse_episode()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reinforcement_Learning",
   "language": "python",
   "name": "reinforcement_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
